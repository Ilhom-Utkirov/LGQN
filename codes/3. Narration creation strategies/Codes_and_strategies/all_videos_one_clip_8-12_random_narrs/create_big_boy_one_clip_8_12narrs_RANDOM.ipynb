{"cells":[{"cell_type":"markdown","metadata":{"id":"lmoup7_Xsbc4"},"source":["# Ego4D create enhanced NLQ dataset for pre-training"]},{"cell_type":"markdown","metadata":{"id":"XBsjg8pN0knX"},"source":["## Download Data and Setup Environment"]},{"cell_type":"markdown","metadata":{"id":"FcNVEU5Z8f5p"},"source":["### **Fill In Your Access Info Here**\n","If you don't have access and secret keys, first sign the Ego4D License at [ego4ddataset.com](https://ego4ddataset.com)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTSvhBsBvnXy"},"outputs":[],"source":["import os\n","os.environ['AWS_ACCESS_KEY_ID'] = \"AKIATEEVKTGZOYNR6VTB\"\n","os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\""]},{"cell_type":"markdown","metadata":{"id":"WcIg7gNx82Bq"},"source":["### **Set up CLIs and Download Annotations + Repo**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-D9Jm-l162m","outputId":"c32e2094-8605-4a3a-ba0a-81a5de3b080a"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 57.8M  100 57.8M    0     0  91.9M      0 --:--:-- --:--:-- --:--:-- 91.8M\n"]}],"source":["# Download the AWS and Ego4D CLIs, then download the annotations locally\n","# Set up the AWS CLI\n","!curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n","!unzip -o awscliv2.zip >/dev/null\n","!sudo ./aws/install >/dev/null 2>&1\n","!aws configure set aws_access_key_id \"$AWS_ACCESS_KEY_ID\" && aws configure set aws_secret_access_key \"$AWS_SECRET_ACCESS_KEY\"\n","!rm \"awscliv2.zip\""]},{"cell_type":"markdown","metadata":{"id":"5tXEDSW50Ebd"},"source":["### Install the ego4d CLI and Download Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Jg6Xt1p-On-a","outputId":"e494ac19-0c67-4983-bc98-dc5957273652"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ego4d\n","  Downloading ego4d-1.7.3.tar.gz (94 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/94.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting boto3 (from ego4d)\n","  Downloading boto3-1.34.133-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ego4d) (4.66.4)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from ego4d) (2024.5.15)\n","Collecting dataclasses-json (from ego4d)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Collecting iopath (from ego4d)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting botocore<1.35.0,>=1.34.133 (from boto3->ego4d)\n","  Downloading botocore-1.34.133-py3-none-any.whl (12.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->ego4d)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->ego4d)\n","  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->ego4d)\n","  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->ego4d)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->ego4d) (4.12.2)\n","Collecting portalocker (from iopath->ego4d)\n","  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.133->boto3->ego4d) (2.8.2)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.133->boto3->ego4d) (2.0.7)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->ego4d) (24.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->ego4d)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.133->boto3->ego4d) (1.16.0)\n","Building wheels for collected packages: ego4d, iopath\n","  Building wheel for ego4d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ego4d: filename=ego4d-1.7.3-py3-none-any.whl size=118250 sha256=24977862cb791ac1d767d72322c59fbf6319bb83632fb69e81361ecd8c84d1dc\n","  Stored in directory: /root/.cache/pip/wheels/65/a8/89/a6187e3bc9a85e81899ab8d5ddc2011c9954d3b6cb84d47e03\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=d867b83e80ec64eba86bbfc1af748ad632b582bb166a48cfd4d47377cffb8fe9\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built ego4d iopath\n","Installing collected packages: portalocker, mypy-extensions, marshmallow, jmespath, typing-inspect, iopath, botocore, s3transfer, dataclasses-json, boto3, ego4d\n","Successfully installed boto3-1.34.133 botocore-1.34.133 dataclasses-json-0.6.7 ego4d-1.7.3 iopath-0.1.10 jmespath-1.0.1 marshmallow-3.21.3 mypy-extensions-1.0.0 portalocker-2.10.0 s3transfer-0.10.2 typing-inspect-0.9.0\n"]}],"source":["# Set up the Ego4D CLI\n","!pip install ego4d"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcKr9i88KMaa","outputId":"9bf87484-d280-48f2-8cb9-018387d47789"},"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets to download: {'annotations'}\n","Download Path: /content/ego4d_data/v1\n","Downloading Ego4D metadata json..\n","Ego4D Metadata: /content/ego4d_data/ego4d.json\n","Checking requested datasets and versions...\n","Created download directory for version 'v1' of dataset: 'annotations' at: /content/ego4d_data/v1/annotations\n","Benchmarks specified but ignored without a benchmarks field in manifest.\n","Retrieving object metadata from S3...\n","100% 31/31 [00:00<00:00, 238.58object/s]\n","Checking if latest file versions are already downloaded...\n","100% 31/31 [00:00<00:00, 88.02file/s]\n","No existing videos to filter.\n","Downloading 31 files..\n","100% 2.50G/2.51G [00:28<00:00, 128MiB/s]Checking file integrity...\n","100% 2.51G/2.51G [00:28<00:00, 93.3MiB/s]\n"]}],"source":["# Download the Ego4D Annotations to ego4d_data/\n","!ego4d --output_directory=\"/content/ego4d_data/\" --datasets annotations --benchmarks nlq -y --version v1"]},{"cell_type":"markdown","metadata":{"id":"LoJPAcJuCwJg"},"source":["## VLP features downloading\n"]},{"cell_type":"code","source":["# Open the file in read mode\n","with open(\"UIDs_of_visual_features.txt\", \"r\") as file:\n","    features_list = [line.strip() for line in file]"],"metadata":{"id":"QJBoVVea28nd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Flbksi8eMBtK"},"source":["Load the json files in pyton dictionaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfgwIsuOBIhM"},"outputs":[],"source":["import json\n","metadata = json.load(open(\"/content/ego4d_data/ego4d.json\"))\n","narration = json.load(open(\"/content/ego4d_data/v1/annotations/narration.json\"))\n","nlq_val = json.load(open(\"/content/ego4d_data/v1/annotations/nlq_val.json\"))"]},{"cell_type":"markdown","source":[],"metadata":{"id":"ETtBhkaJVoOu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RakRb4IpBy0J"},"outputs":[],"source":["videos = metadata.get(\"videos\")\n","clips = metadata.get(\"clips\")"]},{"cell_type":"markdown","metadata":{"id":"-rBhEdtqPsdi"},"source":["Skip the videos that are included in nlq_train and nlq_val"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aA-BTE0XPrZP","outputId":"c192c0ff-4282-43f3-935e-91967586392b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of nlq_val_videos_ids:  247\n"]}],"source":["nlq_val_videos = nlq_val.get(\"videos\")\n","nlq_val_videos_ids= []\n","for item in nlq_val_videos:\n","  nlq_val_videos_ids.append(item[\"video_uid\"])\n","\n","print(\"Number of nlq_val_videos_ids: \",len(nlq_val_videos_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmS9go79R8bq","outputId":"a9df996b-59b0-42e3-b8f7-e30bf9357584"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of nlq_train_videos_ids: 754\n"]}],"source":["nlq_train = json.load(open(\"/content/ego4d_data/v1/annotations/nlq_train.json\"))\n","\n","nlq_train_videos = nlq_train.get(\"videos\")\n","nlq_train_videos_ids= []\n","for item in nlq_train_videos:\n","  nlq_train_videos_ids.append(item[\"video_uid\"])\n","\n","print(\"Number of nlq_train_videos_ids:\", len(nlq_train_videos_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VHqet7tPrbu","outputId":"a2e02ec9-faf3-47ff-fafe-c0bec2a5998e"},"outputs":[{"output_type":"stream","name":"stdout","text":["12283\n"]}],"source":["metadata_clips = metadata.get(\"clips\")\n","metadata_clips_video_ids = []\n","for item in metadata_clips:\n","  metadata_clips_video_ids.append(item[\"video_uid\"])\n","\n","# of videos included in metadata\n","print(len(metadata_clips_video_ids))"]},{"cell_type":"markdown","source":["# In the above code there is duplicates. only unique videos needed\n"],"metadata":{"id":"Wvi1hoURaWuV"}},{"cell_type":"code","source":["unique_video_ids = list(set(metadata_clips_video_ids))\n","\n","# Print the number of unique video IDs,\n","# means only 3878 video has clips\n","print(\"number of videos with clips:\", len(unique_video_ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFo2aa5caPIZ","outputId":"3c7a042c-86c2-4d44-cf8d-db39deb4b911"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of videos with clips: 3878\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVvC6jPkUN74","outputId":"d550ad85-f26b-4911-b81d-85b8f499b5d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["9645\n"]}],"source":["narration_videos = list(narration.keys())\n","print(len(narration_videos))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJUTZwUAPret","outputId":"7dfacfde-a66d-4d13-c2af-9aeb43039a55"},"outputs":[{"output_type":"stream","name":"stdout","text":["allowed_videos: 8891\n"]}],"source":["excluded_videos = nlq_train_videos_ids + nlq_val_videos\n","\n","allowed_videos = [item for item in narration_videos if item not in excluded_videos]\n","print(\"allowed_videos:\", len(allowed_videos))"]},{"cell_type":"markdown","metadata":{"id":"7zzRW15gVMWs"},"source":["So far, we used only narration.json. Now, we will combine the informations from the metadata and narrations to create real json file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aa2Buy61EpBt","outputId":"62f765f2-6499-4186-d0a3-c53055e4573d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of common files: 8839\n"]}],"source":["# Convert the lists to sets\n","set1 = set(features_list)\n","set2 = set(allowed_videos)\n","\n","# Find the intersection of the two sets\n","common_files = set1.intersection(set2)\n","\n","# Get the count of the common files\n","common_files_count = len(common_files)\n","\n","# Print the count\n","print(\"Number of common files:\", common_files_count)\n","\n","common_files = list(common_files)"]},{"cell_type":"markdown","source":["# Creation of main dataset\n","\n","* This script processes video metadata and narration data to build a dictionary of videos, each containing clips and their corresponding annotations, while applying filters on the number of narrations and clips.\n"],"metadata":{"id":"nxgeFO55TfyK"}},{"cell_type":"markdown","source":["This is Ilhom's version:"],"metadata":{"id":"d1HUnKhbalsM"}},{"cell_type":"code","source":["# big_dictionary = {\"videos\":[]}\n","# filter_max_narrations = 15\n","# filter_max_clips = 1\n","\n","# #total 260 MB of data if we use all stuff\n","# counter = 0\n","# skipped_videos = 0\n","\n","# for video_uid in narration:\n","#   # Skip videos not in common_files\n","#   if video_uid not in common_files:\n","#     skipped_videos = skipped_videos+1\n","#     continue\n","\n","#   metadata_clips_as_dict = metadata.get(\"clips\")\n","\n","#   # get clips for this video\n","#   metadata_clips = [item for item in metadata_clips_as_dict if item.get(\"video_uid\") == video_uid]\n","#   if len(metadata_clips) == 0:\n","#     continue\n","\n","#   list_of_clips = []\n","#   for (counter_clips, metadata_video) in enumerate(metadata_clips):\n","#     dic_with_metadata = {\n","#         \"clip_uid\" : metadata_video[\"clip_uid\"],\n","#         \"video_start_sec\" : metadata_video[\"video_start_sec\"],\n","#         \"video_end_sec\" : metadata_video[\"video_end_sec\"],\n","#         \"annotations\":[]\n","#     }\n","\n","#     video = narration[video_uid]\n","#     if not video:\n","#       continue\n","#     narrator_1 = video.get(\"narration_pass_1\")\n","#     if not narrator_1:\n","#       continue\n","#     narrations_for_vid = (narrator_1.get(\"narrations\"))\n","#     if len(narrations_for_vid) == 0:\n","#       continue\n","\n","#     language_queries = []\n","#     # Skip the last narration for each video\n","#     for i in range(len(narrations_for_vid)-1):\n","#       # check if narrations are inside the clip duration\n","#       flag_is_clip_last = len(metadata_clips)\n","#       flag_is_narration_inside_clip = (\n","#         metadata_video[\"video_start_sec\"] <= narrations_for_vid[i][\"timestamp_sec\"] and\n","#         metadata_video[\"video_end_sec\"] >= narrations_for_vid[i][\"timestamp_sec\"]\n","#       )\n","#       if flag_is_narration_inside_clip:\n","#         # Determine end time of narration\n","#         end_narration_time = None\n","#         # scenario1 - check if it is lower than clip end time\n","#         if narrations_for_vid[i+1][\"timestamp_sec\"] >= metadata_video[\"video_end_sec\"]:\n","#           end_narration_time = metadata_video[\"video_end_sec\"] - metadata_video[\"video_start_sec\"]\n","#         else:\n","#           # scenario2 - take next narration start\n","#           end_narration_time = narrations_for_vid[i+1][\"timestamp_sec\"] - metadata_video[\"video_start_sec\"]\n","\n","#         dic = {\n","#             \"clip_start_sec\" : narrations_for_vid[i][\"timestamp_sec\"] - metadata_video[\"video_start_sec\"] ,\n","#             \"clip_end_sec\" : end_narration_time,\n","#             \"query\": narrations_for_vid[i][\"narration_text\"]\n","#         }\n","#         if len(language_queries) < filter_max_narrations:\n","#           language_queries.append(dic)\n","\n","#     annotation_uid = narrations_for_vid[0][\"annotation_uid\"]\n","#     dic_with_metadata[\"annotations\"].append({\n","#         \"language_queries\":language_queries,\n","#         \"annotation_uid\":annotation_uid\n","#         })\n","\n","#     # Append clip to list if it has valid annotations\n","#     if len(language_queries) != 0 and len(list_of_clips) <= filter_max_clips:\n","#       list_of_clips.append(dic_with_metadata)\n","\n","#   # Append video to final dictionary if it has valid clips\n","#   if len(list_of_clips) != 0:\n","#     ids_dictionary = {\n","#         \"video_uid\" : video_uid,\n","#         \"clips\": list_of_clips\n","#     }\n","#     big_dictionary[\"videos\"].append(ids_dictionary)\n","#   counter = counter + 1\n","\n","\n","# print(counter)"],"metadata":{"id":"KqqSPznYY3hD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some of the  problems with Ilhom's code:\n","\n","Let's assume filter_max_narrations = 12. The code simply selects only the clips that have narrations less that 12. If there are 25 narrations in the clip, it doesn't take 12 and discard the others but discards all narrations. The same applies for filter_max_clips. If a video has one clip, it works perfectly, but if a video has more than one clip, it doesn't use none. counter_clips and flag_is_clip_last are introduced but aren't used."],"metadata":{"id":"1yNRAg1Le7DJ"}},{"cell_type":"code","source":["def get_narrations_for_current_clip(video_start_sec, video_end_sec, narrations_dict):\n","  \"\"\"\n","  This method takes as input the start_sec, the end_sec of the clip and\n","  all narrations from narration.json that correspond to the video.\n","\n","  The method outputs a list of dictionaries corresponding to 'language_queries'\n","  from the NLQ schema containing only narrations from the current clip.\n","\n","  I do it this way because later is very easy to change the logic of the strategy\n","  when we already have only the narrations from the current list\n","  \"\"\"\n","\n","  LENGTH = len(narrations_dict)\n","\n","\n","  result_list = []\n","\n","  i = 0\n","\n","  # We iterate through the narrations until we find the first narration for the clip\n","  while LENGTH> i+1 and narrations_dict[i]['timestamp_sec']< video_start_sec:\n","    i = i + 1\n","\n","\n","  #We iterate through the remaining part, create and add dictionaries to the list\n","  #until we find the last narration that is fully inside the clip\n","  while LENGTH> i+1 and narrations_dict[i+1]['timestamp_sec']< video_end_sec:\n","    dic = {\n","            \"clip_start_sec\" : narrations_dict[i][\"timestamp_sec\"] - video_start_sec ,\n","            \"clip_end_sec\" : narrations_dict[i+1][\"timestamp_sec\"] - video_start_sec,\n","            \"query\": narrations_dict[i][\"narration_text\"]\n","        }\n","    result_list.append(dic)\n","    i = i + 1\n","\n","  return result_list"],"metadata":{"id":"YyDbKGu-m928"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def select_random_queries(narrations, number_of_narrations):\n","  # this method returns a list with two consecutive narrations with equal length\n","  possible_positions = list(range(len(narrations)))\n","\n","  samples  = random.sample(possible_positions, number_of_narrations)\n","  samples.sort()\n","\n","  language_queries = []\n","  for index in samples:\n","    language_queries.append(narrations[index])\n","\n","  return language_queries\n"],"metadata":{"id":"zuHG8U63kHz3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we will get the narrations from here and then we apply the random stuff, and selection of the narrations, directly from the narrations for the current video"],"metadata":{"id":"UcDTeqJcqFME"}},{"cell_type":"code","source":["import random\n","random.seed(12)"],"metadata":{"id":"gbYQanflVkXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set parameters\n","min_narr_per_clip = 8  # min number of narrations per clip\n","max_narr_per_clip = 12 # man number of narrations per clip\n","filter_max_clips = 1   # max used number of clips per video\n","\n","\n","metadata_clips_as_dict = metadata.get(\"clips\")\n","big_dictionary = {\"videos\":[]}\n","videos_counter = 0\n","skipped_videos = 0\n","\n","for video_uid in narration:\n","  # Skip videos not in common_files\n","  if video_uid not in common_files:\n","    skipped_videos = skipped_videos+1\n","    continue\n","\n","  # get clips for this video\n","  metadata_clips = [item for item in metadata_clips_as_dict if item.get(\"video_uid\") == video_uid]\n","  if len(metadata_clips) == 0:\n","    continue\n","\n","  list_of_clips = []\n","  # here we filter out how many clips we are going to use, starting from the first clip\n","  for clip in metadata_clips[0:filter_max_clips]:\n","    dic_with_metadata = {\n","        \"clip_uid\" : clip[\"clip_uid\"],\n","        \"video_start_sec\" : clip[\"video_start_sec\"],\n","        \"video_end_sec\" : clip[\"video_end_sec\"],\n","        \"annotations\":[]\n","    }\n","\n","    video = narration[video_uid]\n","    if not video:\n","      continue\n","    narrator_1 = video.get(\"narration_pass_1\")\n","    if not narrator_1:\n","      continue\n","    narrations_for_vid = (narrator_1.get(\"narrations\"))\n","    if len(narrations_for_vid) < 3: # we directly skip these narrations\n","      continue\n","\n","    # Generate a random number corresponding to the count of narrations for the current clip, for example 12\n","    number_of_narrations = random.randint(min_narr_per_clip, max_narr_per_clip)\n","\n","    # Get all narrations for the current clip in the correct format\n","    narrations_for_clip = get_narrations_for_current_clip(clip[\"video_start_sec\"], clip[\"video_end_sec\"], narrations_for_vid)\n","\n","    # if the number of narrations available is less or equal to the wanted number, just use all available narrations for the clip\n","    if len(narrations_for_clip) <= number_of_narrations:\n","      language_queries = narrations_for_clip\n","    # else select a random consecutive subset of the narrations\n","    else:\n","      #position = random.randint(0, len(narrations_for_clip) - number_of_narrations +1)\n","      #language_queries = narrations_for_clip[position:position +number_of_narrations]\n","      language_queries = select_random_queries(narrations_for_clip, number_of_narrations)\n","\n","    if len(language_queries) < 1:\n","      continue\n","\n","    annotation_uid = narrations_for_vid[0][\"annotation_uid\"]\n","    dic_with_metadata[\"annotations\"].append({\n","        \"language_queries\":language_queries,\n","        \"annotation_uid\":annotation_uid\n","        })\n","\n","\n","    list_of_clips.append(dic_with_metadata)\n","\n","  # Append video to final dictionary if it has valid clips\n","  if len(list_of_clips) != 0:\n","    ids_dictionary = {\n","        \"video_uid\" : video_uid,\n","        \"clips\": list_of_clips\n","    }\n","    big_dictionary[\"videos\"].append(ids_dictionary)\n","  videos_counter = videos_counter + 1\n","\n","print(\"Total number of videos:\", videos_counter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SH1GJ8ad0RB","outputId":"ed021d5e-2a12-49e3-d487-a66e879b0e10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of videos: 3123\n"]}]},{"cell_type":"code","source":["random.shuffle(big_dictionary[\"videos\"])"],"metadata":{"id":"0DzU69SypJOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Check if everything is correct"],"metadata":{"id":"ulVOhX6_WKzW"}},{"cell_type":"code","source":["number_of_queries_per_video = []\n","number_of_queries_per_clip = []\n","number_of_queries_per_annotation = []\n","anns = []\n","total_number_of_queries = 0\n","\n","for vid in big_dictionary[\"videos\"]:\n","\n","    number_of_queries_v = 0\n","    for clip in vid[\"clips\"]:\n","\n","        number_of_queries_c = 0\n","        for ann in clip[\"annotations\"]:\n","\n","            num_of_q_ann = 0\n","            for query in ann[\"language_queries\"]:\n","                total_number_of_queries = total_number_of_queries + 1\n","                num_of_q_ann = num_of_q_ann +1\n","                number_of_queries_c = number_of_queries_c +1\n","                number_of_queries_v = number_of_queries_v +1\n","            number_of_queries_per_annotation.append(num_of_q_ann)\n","        number_of_queries_per_clip.append(number_of_queries_c)\n","    number_of_queries_per_video.append(number_of_queries_v)\n","\n","print(\"Total number of queries:\", total_number_of_queries)\n","print(\"number of clips with 0 language queries\", number_of_queries_per_clip.count(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfYCg7JtWDdX","outputId":"6ca2f3f1-73b2-4cff-a40f-92621a137ae5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of queries: 21974\n","number of clips with 0 language queries 0\n"]}]},{"cell_type":"markdown","source":["# Saving the dataset"],"metadata":{"id":"bY75f8eKWEdy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lDzytYG1TvJ"},"outputs":[],"source":["# Save the modified dictionary back to the JSON file\n","with open('big_boy_all_videos_one_clip_8-12nars_RANDOM.json', 'w') as file:\n","    json.dump(big_dictionary, file, indent=4)"]},{"cell_type":"code","source":[],"metadata":{"id":"j2pNAMwiLpD2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}