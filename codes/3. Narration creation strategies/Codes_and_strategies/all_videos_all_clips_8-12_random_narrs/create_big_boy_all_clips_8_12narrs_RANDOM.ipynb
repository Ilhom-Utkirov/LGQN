{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmoup7_Xsbc4"
      },
      "source": [
        "# Ego4D create enhanced NLQ dataset for pre-training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBsjg8pN0knX"
      },
      "source": [
        "## Download Data and Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcNVEU5Z8f5p"
      },
      "source": [
        "### **Fill In Your Access Info Here**\n",
        "If you don't have access and secret keys, first sign the Ego4D License at [ego4ddataset.com](https://ego4ddataset.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lTSvhBsBvnXy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIATEEVKTGZJQANLWO7\"\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIg7gNx82Bq"
      },
      "source": [
        "### **Set up CLIs and Download Annotations + Repo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-D9Jm-l162m",
        "outputId": "a0021e93-db2b-4d76-ad93-d69018e66edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 57.8M  100 57.8M    0     0   142M      0 --:--:-- --:--:-- --:--:--  142M\n"
          ]
        }
      ],
      "source": [
        "# Download the AWS and Ego4D CLIs, then download the annotations locally\n",
        "# Set up the AWS CLI\n",
        "!curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "!unzip -o awscliv2.zip >/dev/null\n",
        "!sudo ./aws/install >/dev/null 2>&1\n",
        "!aws configure set aws_access_key_id \"$AWS_ACCESS_KEY_ID\" && aws configure set aws_secret_access_key \"$AWS_SECRET_ACCESS_KEY\"\n",
        "!rm \"awscliv2.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tXEDSW50Ebd"
      },
      "source": [
        "### Install the ego4d CLI and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Jg6Xt1p-On-a",
        "outputId": "ce3d786e-e766-4494-c17c-88805327e05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ego4d in /usr/local/lib/python3.10/dist-packages (1.7.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from ego4d) (1.34.133)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ego4d) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from ego4d) (2024.5.15)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from ego4d) (0.6.7)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from ego4d) (0.1.10)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.133 in /usr/local/lib/python3.10/dist-packages (from boto3->ego4d) (1.34.133)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->ego4d) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->ego4d) (0.10.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->ego4d) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->ego4d) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->ego4d) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->ego4d) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.133->boto3->ego4d) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.133->boto3->ego4d) (2.0.7)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->ego4d) (24.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->ego4d) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.133->boto3->ego4d) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Set up the Ego4D CLI\n",
        "!pip install ego4d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcKr9i88KMaa",
        "outputId": "44abe493-b804-4fcd-9386-4b45060bafdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets to download: {'annotations'}\n",
            "Download Path: /content/ego4d_data/v1\n",
            "Ego4D Metadata: /content/ego4d_data/ego4d.json\n",
            "Checking requested datasets and versions...\n",
            "Created download directory for version 'v1' of dataset: 'annotations' at: /content/ego4d_data/v1/annotations\n",
            "Benchmarks specified but ignored without a benchmarks field in manifest.\n",
            "Retrieving object metadata from S3...\n",
            "100% 31/31 [00:00<00:00, 410.32object/s]\n",
            "Checking if latest file versions are already downloaded...\n",
            "100% 31/31 [00:01<00:00, 29.23file/s]\n",
            "Filtered 31/31 existing videos for download.\n",
            "The latest versions of all requested videos already exist in the output directories under:\n",
            "/content/ego4d_data\n"
          ]
        }
      ],
      "source": [
        "# Download the Ego4D Annotations to ego4d_data/\n",
        "!ego4d --output_directory=\"/content/ego4d_data/\" --datasets annotations --benchmarks nlq -y --version v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoJPAcJuCwJg"
      },
      "source": [
        "## VLP features downloading\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file in read mode\n",
        "with open(\"UIDs_of_visual_features.txt\", \"r\") as file:\n",
        "    features_list = [line.strip() for line in file]"
      ],
      "metadata": {
        "id": "QJBoVVea28nd"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flbksi8eMBtK"
      },
      "source": [
        "Load the json files in pyton dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "IfgwIsuOBIhM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "metadata = json.load(open(\"/content/ego4d_data/ego4d.json\"))\n",
        "narration = json.load(open(\"/content/ego4d_data/v1/annotations/narration.json\"))\n",
        "nlq_val = json.load(open(\"/content/ego4d_data/v1/annotations/nlq_val.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ETtBhkaJVoOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "RakRb4IpBy0J"
      },
      "outputs": [],
      "source": [
        "videos = metadata.get(\"videos\")\n",
        "clips = metadata.get(\"clips\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rBhEdtqPsdi"
      },
      "source": [
        "Skip the videos that are included in nlq_train and nlq_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA-BTE0XPrZP",
        "outputId": "a0c51280-2c17-4c6d-ccce-4a1dc6239e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nlq_val_videos_ids:  247\n"
          ]
        }
      ],
      "source": [
        "nlq_val_videos = nlq_val.get(\"videos\")\n",
        "nlq_val_videos_ids= []\n",
        "for item in nlq_val_videos:\n",
        "  nlq_val_videos_ids.append(item[\"video_uid\"])\n",
        "\n",
        "print(\"Number of nlq_val_videos_ids: \",len(nlq_val_videos_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmS9go79R8bq",
        "outputId": "69ab35b5-c0fd-4b1c-e15f-4d2302a6b818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nlq_train_videos_ids: 754\n"
          ]
        }
      ],
      "source": [
        "nlq_train = json.load(open(\"/content/ego4d_data/v1/annotations/nlq_train.json\"))\n",
        "\n",
        "nlq_train_videos = nlq_train.get(\"videos\")\n",
        "nlq_train_videos_ids= []\n",
        "for item in nlq_train_videos:\n",
        "  nlq_train_videos_ids.append(item[\"video_uid\"])\n",
        "\n",
        "print(\"Number of nlq_train_videos_ids:\", len(nlq_train_videos_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VHqet7tPrbu",
        "outputId": "4ff02277-1795-4eb3-fa6b-04962f4d047f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12283\n"
          ]
        }
      ],
      "source": [
        "metadata_clips = metadata.get(\"clips\")\n",
        "metadata_clips_video_ids = []\n",
        "for item in metadata_clips:\n",
        "  metadata_clips_video_ids.append(item[\"video_uid\"])\n",
        "\n",
        "# of videos included in metadata\n",
        "print(len(metadata_clips_video_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In the above code there is duplicates. only unique videos needed\n"
      ],
      "metadata": {
        "id": "Wvi1hoURaWuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_video_ids = list(set(metadata_clips_video_ids))\n",
        "\n",
        "# Print the number of unique video IDs,\n",
        "# means only 3878 video has clips\n",
        "print(\"number of videos with clips:\", len(unique_video_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFo2aa5caPIZ",
        "outputId": "7df43d29-76cf-4d6c-809b-835bb8b8f6ce"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of videos with clips: 3878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVvC6jPkUN74",
        "outputId": "18db6ff5-949a-492a-a047-e2dcefdfe563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9645\n"
          ]
        }
      ],
      "source": [
        "narration_videos = list(narration.keys())\n",
        "print(len(narration_videos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJUTZwUAPret",
        "outputId": "facc8fe0-a3b5-4315-eb31-19a491eac2e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allowed_videos: 8891\n"
          ]
        }
      ],
      "source": [
        "excluded_videos = nlq_train_videos_ids + nlq_val_videos\n",
        "\n",
        "allowed_videos = [item for item in narration_videos if item not in excluded_videos]\n",
        "print(\"allowed_videos:\", len(allowed_videos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zzRW15gVMWs"
      },
      "source": [
        "So far, we used only narration.json. Now, we will combine the informations from the metadata and narrations to create real json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2Buy61EpBt",
        "outputId": "5d40e1e4-a1ed-42dd-9eac-6ba39954e71f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common files: 8839\n"
          ]
        }
      ],
      "source": [
        "# Convert the lists to sets\n",
        "set1 = set(features_list)\n",
        "set2 = set(allowed_videos)\n",
        "\n",
        "# Find the intersection of the two sets\n",
        "common_files = set1.intersection(set2)\n",
        "\n",
        "# Get the count of the common files\n",
        "common_files_count = len(common_files)\n",
        "\n",
        "# Print the count\n",
        "print(\"Number of common files:\", common_files_count)\n",
        "\n",
        "common_files = list(common_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of main dataset\n",
        "\n",
        "* This script processes video metadata and narration data to build a dictionary of videos, each containing clips and their corresponding annotations, while applying filters on the number of narrations and clips.\n"
      ],
      "metadata": {
        "id": "nxgeFO55TfyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is Ilhom's version:"
      ],
      "metadata": {
        "id": "d1HUnKhbalsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# big_dictionary = {\"videos\":[]}\n",
        "# filter_max_narrations = 15\n",
        "# filter_max_clips = 1\n",
        "\n",
        "# #total 260 MB of data if we use all stuff\n",
        "# counter = 0\n",
        "# skipped_videos = 0\n",
        "\n",
        "# for video_uid in narration:\n",
        "#   # Skip videos not in common_files\n",
        "#   if video_uid not in common_files:\n",
        "#     skipped_videos = skipped_videos+1\n",
        "#     continue\n",
        "\n",
        "#   metadata_clips_as_dict = metadata.get(\"clips\")\n",
        "\n",
        "#   # get clips for this video\n",
        "#   metadata_clips = [item for item in metadata_clips_as_dict if item.get(\"video_uid\") == video_uid]\n",
        "#   if len(metadata_clips) == 0:\n",
        "#     continue\n",
        "\n",
        "#   list_of_clips = []\n",
        "#   for (counter_clips, metadata_video) in enumerate(metadata_clips):\n",
        "#     dic_with_metadata = {\n",
        "#         \"clip_uid\" : metadata_video[\"clip_uid\"],\n",
        "#         \"video_start_sec\" : metadata_video[\"video_start_sec\"],\n",
        "#         \"video_end_sec\" : metadata_video[\"video_end_sec\"],\n",
        "#         \"annotations\":[]\n",
        "#     }\n",
        "\n",
        "#     video = narration[video_uid]\n",
        "#     if not video:\n",
        "#       continue\n",
        "#     narrator_1 = video.get(\"narration_pass_1\")\n",
        "#     if not narrator_1:\n",
        "#       continue\n",
        "#     narrations_for_vid = (narrator_1.get(\"narrations\"))\n",
        "#     if len(narrations_for_vid) == 0:\n",
        "#       continue\n",
        "\n",
        "#     language_queries = []\n",
        "#     # Skip the last narration for each video\n",
        "#     for i in range(len(narrations_for_vid)-1):\n",
        "#       # check if narrations are inside the clip duration\n",
        "#       flag_is_clip_last = len(metadata_clips)\n",
        "#       flag_is_narration_inside_clip = (\n",
        "#         metadata_video[\"video_start_sec\"] <= narrations_for_vid[i][\"timestamp_sec\"] and\n",
        "#         metadata_video[\"video_end_sec\"] >= narrations_for_vid[i][\"timestamp_sec\"]\n",
        "#       )\n",
        "#       if flag_is_narration_inside_clip:\n",
        "#         # Determine end time of narration\n",
        "#         end_narration_time = None\n",
        "#         # scenario1 - check if it is lower than clip end time\n",
        "#         if narrations_for_vid[i+1][\"timestamp_sec\"] >= metadata_video[\"video_end_sec\"]:\n",
        "#           end_narration_time = metadata_video[\"video_end_sec\"] - metadata_video[\"video_start_sec\"]\n",
        "#         else:\n",
        "#           # scenario2 - take next narration start\n",
        "#           end_narration_time = narrations_for_vid[i+1][\"timestamp_sec\"] - metadata_video[\"video_start_sec\"]\n",
        "\n",
        "#         dic = {\n",
        "#             \"clip_start_sec\" : narrations_for_vid[i][\"timestamp_sec\"] - metadata_video[\"video_start_sec\"] ,\n",
        "#             \"clip_end_sec\" : end_narration_time,\n",
        "#             \"query\": narrations_for_vid[i][\"narration_text\"]\n",
        "#         }\n",
        "#         if len(language_queries) < filter_max_narrations:\n",
        "#           language_queries.append(dic)\n",
        "\n",
        "#     annotation_uid = narrations_for_vid[0][\"annotation_uid\"]\n",
        "#     dic_with_metadata[\"annotations\"].append({\n",
        "#         \"language_queries\":language_queries,\n",
        "#         \"annotation_uid\":annotation_uid\n",
        "#         })\n",
        "\n",
        "#     # Append clip to list if it has valid annotations\n",
        "#     if len(language_queries) != 0 and len(list_of_clips) <= filter_max_clips:\n",
        "#       list_of_clips.append(dic_with_metadata)\n",
        "\n",
        "#   # Append video to final dictionary if it has valid clips\n",
        "#   if len(list_of_clips) != 0:\n",
        "#     ids_dictionary = {\n",
        "#         \"video_uid\" : video_uid,\n",
        "#         \"clips\": list_of_clips\n",
        "#     }\n",
        "#     big_dictionary[\"videos\"].append(ids_dictionary)\n",
        "#   counter = counter + 1\n",
        "\n",
        "\n",
        "# print(counter)"
      ],
      "metadata": {
        "id": "KqqSPznYY3hD"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the  problems with Ilhom's code:\n",
        "\n",
        "Let's assume filter_max_narrations = 12. The code simply selects only the clips that have narrations less that 12. If there are 25 narrations in the clip, it doesn't take 12 and discard the others but discards all narrations. The same applies for filter_max_clips. If a video has one clip, it works perfectly, but if a video has more than one clip, it doesn't use none. counter_clips and flag_is_clip_last are introduced but aren't used."
      ],
      "metadata": {
        "id": "1yNRAg1Le7DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_narrations_for_current_clip(video_start_sec, video_end_sec, narrations_dict):\n",
        "  \"\"\"\n",
        "  This method takes as input the start_sec, the end_sec of the clip and\n",
        "  all narrations from narration.json that correspond to the video.\n",
        "\n",
        "  The method outputs a list of dictionaries corresponding to 'language_queries'\n",
        "  from the NLQ schema containing only narrations from the current clip.\n",
        "\n",
        "  I do it this way because later is very easy to change the logic of the strategy\n",
        "  when we already have only the narrations from the current list\n",
        "  \"\"\"\n",
        "\n",
        "  LENGTH = len(narrations_dict)\n",
        "\n",
        "\n",
        "  result_list = []\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  # We iterate through the narrations until we find the first narration for the clip\n",
        "  while LENGTH> i+1 and narrations_dict[i]['timestamp_sec']< video_start_sec:\n",
        "    i = i + 1\n",
        "\n",
        "\n",
        "  #We iterate through the remaining part, create and add dictionaries to the list\n",
        "  #until we find the last narration that is fully inside the clip\n",
        "  while LENGTH> i+1 and narrations_dict[i+1]['timestamp_sec']< video_end_sec:\n",
        "    dic = {\n",
        "            \"clip_start_sec\" : narrations_dict[i][\"timestamp_sec\"] - video_start_sec ,\n",
        "            \"clip_end_sec\" : narrations_dict[i+1][\"timestamp_sec\"] - video_start_sec,\n",
        "            \"query\": narrations_dict[i][\"narration_text\"]\n",
        "        }\n",
        "    result_list.append(dic)\n",
        "    i = i + 1\n",
        "\n",
        "  return result_list"
      ],
      "metadata": {
        "id": "YyDbKGu-m928"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_random_queries(narrations, number_of_narrations):\n",
        "  # this method returns a list with two consecutive narrations with equal length\n",
        "  possible_positions = list(range(len(narrations)))\n",
        "\n",
        "  samples  = random.sample(possible_positions, number_of_narrations)\n",
        "  samples.sort()\n",
        "\n",
        "  language_queries = []\n",
        "  for index in samples:\n",
        "    language_queries.append(narrations[index])\n",
        "\n",
        "  return language_queries\n"
      ],
      "metadata": {
        "id": "zuHG8U63kHz3"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will get the narrations from here and then we apply the random stuff, and selection of the narrations, directly from the narrations for the current video"
      ],
      "metadata": {
        "id": "UcDTeqJcqFME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(12)"
      ],
      "metadata": {
        "id": "gbYQanflVkXt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set parameters\n",
        "min_narr_per_clip = 8  # min number of narrations per clip\n",
        "max_narr_per_clip = 12 # man number of narrations per clip\n",
        "filter_max_clips = 100000   # max used number of clips per video\n",
        "\n",
        "\n",
        "metadata_clips_as_dict = metadata.get(\"clips\")\n",
        "big_dictionary = {\"videos\":[]}\n",
        "videos_counter = 0\n",
        "skipped_videos = 0\n",
        "\n",
        "for video_uid in narration:\n",
        "  # Skip videos not in common_files\n",
        "  if video_uid not in common_files:\n",
        "    skipped_videos = skipped_videos+1\n",
        "    continue\n",
        "\n",
        "  # get clips for this video\n",
        "  metadata_clips = [item for item in metadata_clips_as_dict if item.get(\"video_uid\") == video_uid]\n",
        "  if len(metadata_clips) == 0:\n",
        "    continue\n",
        "\n",
        "  list_of_clips = []\n",
        "  # here we filter out how many clips we are going to use, starting from the first clip\n",
        "  for clip in metadata_clips[0:filter_max_clips]:\n",
        "    dic_with_metadata = {\n",
        "        \"clip_uid\" : clip[\"clip_uid\"],\n",
        "        \"video_start_sec\" : clip[\"video_start_sec\"],\n",
        "        \"video_end_sec\" : clip[\"video_end_sec\"],\n",
        "        \"annotations\":[]\n",
        "    }\n",
        "\n",
        "    video = narration[video_uid]\n",
        "    if not video:\n",
        "      continue\n",
        "    narrator_1 = video.get(\"narration_pass_1\")\n",
        "    if not narrator_1:\n",
        "      continue\n",
        "    narrations_for_vid = (narrator_1.get(\"narrations\"))\n",
        "    if len(narrations_for_vid) < 3: # we directly skip these narrations\n",
        "      continue\n",
        "\n",
        "    # Generate a random number corresponding to the count of narrations for the current clip, for example 12\n",
        "    number_of_narrations = random.randint(min_narr_per_clip, max_narr_per_clip)\n",
        "\n",
        "    # Get all narrations for the current clip in the correct format\n",
        "    narrations_for_clip = get_narrations_for_current_clip(clip[\"video_start_sec\"], clip[\"video_end_sec\"], narrations_for_vid)\n",
        "\n",
        "    # if the number of narrations available is less or equal to the wanted number, just use all available narrations for the clip\n",
        "    if len(narrations_for_clip) <= number_of_narrations:\n",
        "      language_queries = narrations_for_clip\n",
        "    # else select a random consecutive subset of the narrations\n",
        "    else:\n",
        "      #position = random.randint(0, len(narrations_for_clip) - number_of_narrations +1)\n",
        "      #language_queries = narrations_for_clip[position:position +number_of_narrations]\n",
        "      language_queries = select_random_queries(narrations_for_clip, number_of_narrations)\n",
        "\n",
        "    if len(language_queries) < 1:\n",
        "      continue\n",
        "\n",
        "    annotation_uid = narrations_for_vid[0][\"annotation_uid\"]\n",
        "    dic_with_metadata[\"annotations\"].append({\n",
        "        \"language_queries\":language_queries,\n",
        "        \"annotation_uid\":annotation_uid\n",
        "        })\n",
        "\n",
        "\n",
        "    list_of_clips.append(dic_with_metadata)\n",
        "\n",
        "  # Append video to final dictionary if it has valid clips\n",
        "  if len(list_of_clips) != 0:\n",
        "    ids_dictionary = {\n",
        "        \"video_uid\" : video_uid,\n",
        "        \"clips\": list_of_clips\n",
        "    }\n",
        "    big_dictionary[\"videos\"].append(ids_dictionary)\n",
        "  videos_counter = videos_counter + 1\n",
        "\n",
        "print(\"Total number of videos:\", videos_counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SH1GJ8ad0RB",
        "outputId": "1421f22b-ec93-48f2-c2c1-d42c62bce227"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of videos: 3123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(big_dictionary[\"videos\"])"
      ],
      "metadata": {
        "id": "0DzU69SypJOx"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if everything is correct"
      ],
      "metadata": {
        "id": "ulVOhX6_WKzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_queries_per_video = []\n",
        "number_of_queries_per_clip = []\n",
        "number_of_queries_per_annotation = []\n",
        "anns = []\n",
        "total_number_of_queries = 0\n",
        "\n",
        "for vid in big_dictionary[\"videos\"]:\n",
        "\n",
        "    number_of_queries_v = 0\n",
        "    for clip in vid[\"clips\"]:\n",
        "\n",
        "        number_of_queries_c = 0\n",
        "        for ann in clip[\"annotations\"]:\n",
        "\n",
        "            num_of_q_ann = 0\n",
        "            for query in ann[\"language_queries\"]:\n",
        "                total_number_of_queries = total_number_of_queries + 1\n",
        "                num_of_q_ann = num_of_q_ann +1\n",
        "                number_of_queries_c = number_of_queries_c +1\n",
        "                number_of_queries_v = number_of_queries_v +1\n",
        "            number_of_queries_per_annotation.append(num_of_q_ann)\n",
        "        number_of_queries_per_clip.append(number_of_queries_c)\n",
        "    number_of_queries_per_video.append(number_of_queries_v)\n",
        "\n",
        "print(\"Total number of queries:\", total_number_of_queries)\n",
        "print(\"number of clips with 0 language queries\", number_of_queries_per_clip.count(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfYCg7JtWDdX",
        "outputId": "e1e1e03f-cdb1-409a-9bb2-5eb0be69284b"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of queries: 64904\n",
            "number of clips with 0 language queries 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the dataset"
      ],
      "metadata": {
        "id": "bY75f8eKWEdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "6lDzytYG1TvJ"
      },
      "outputs": [],
      "source": [
        "# Save the modified dictionary back to the JSON file\n",
        "with open('big_boy_all_videos_all_clips_8-12nars_RANDOM.json', 'w') as file:\n",
        "    json.dump(big_dictionary, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2pNAMwiLpD2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGwJO_5YPo4j"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "91lab5MSXZjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Total number of clips in general"
      ],
      "metadata": {
        "id": "K1z43XRpZCoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open('/content/big_boy_all_videos_all_clips_8-12nars_RANDOM.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Function to count the total number of clips for all videos\n",
        "def count_total_clips(data):\n",
        "    total_clips = 0\n",
        "    for video in data['videos']:\n",
        "        total_clips += len(video.get('clips', []))\n",
        "    return total_clips\n",
        "\n",
        "# Count the total number of clips for all videos\n",
        "total_clip_count = count_total_clips(data)\n",
        "print(\"Total number of clips for all videos:\", total_clip_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7DnrIPsZGyN",
        "outputId": "7305aa9c-092f-45fd-a465-cc354a5163c1"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of clips for all videos: 6696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of clips which has MINUS SIGN in\n",
        "clip_start_sec or clip_end_sec"
      ],
      "metadata": {
        "id": "P3rKGEsOXZ1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open('/content/big_boy_all_videos_all_clips_8-12nars_RANDOM.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Function to count unique clips with negative \"clip_start_sec\" or \"clip_end_sec\"\n",
        "def count_negative_clips(data):\n",
        "    unique_clips = set()\n",
        "    for video in data['videos']:\n",
        "        for clip in video.get('clips', []):\n",
        "            for annotation in clip.get('annotations', []):\n",
        "                for query in annotation.get('language_queries', []):\n",
        "                    clip_start_sec = query.get('clip_start_sec', 0)\n",
        "                    clip_end_sec = query.get('clip_end_sec', 0)\n",
        "                    if clip_start_sec < 0 or clip_end_sec < 0:\n",
        "                        unique_clips.add(clip['clip_uid'])\n",
        "    return len(unique_clips)\n",
        "\n",
        "# Count the unique clips with negative \"clip_start_sec\" or \"clip_end_sec\"\n",
        "negative_clip_count = count_negative_clips(data)\n",
        "print(\"Number of unique clips with negative clip_start_sec or clip_end_sec:\", negative_clip_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtu91HqDYN7W",
        "outputId": "c08f1b4a-e967-42ab-a0e4-99071ea00035"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique clips with negative clip_start_sec or clip_end_sec: 228\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ulVOhX6_WKzW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}